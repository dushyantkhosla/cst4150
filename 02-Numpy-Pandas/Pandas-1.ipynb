{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "# The Data Science Process\n",
    "\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "A quick look at the OSEMN process for doing data science\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/ds_process_2.png\" alt=\"Fig 3-2 OSEMN Things\" style=\"width: 1000px\">\n",
    "</center>\n",
    "\n",
    "\n",
    "Broadly, it consists of the following steps\n",
    "\n",
    "| Step          | Task                                     |\n",
    "| :------------ | :--------------------------------------- |\n",
    "| 1.  Question  | Clearly define the business problem      |\n",
    "| 2.  Get Data  | Obtain data from internal/external sources, APIs |\n",
    "| 3.  Wrangle   | Clean messy data. Engineer features. Summarize, aggregate data. |\n",
    "| 4.  Explore   | Visualize distributions. Investigate relationships. Build intuition for subsequent steps. |\n",
    "| 5.  Model     | Build and Tune models. Select the best from competing statistical models. |\n",
    "| 6.  Interpret | Assess model performance on out-of-sample data. Understand results. Draw Insights. |\n",
    "| 7.  Deploy    | Productionalize your analysis. Build a data product. |\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we will focus on **#2 - Get Data** and **#3 - Wrangle** above. \n",
    "But before we begin, a word of caution. \n",
    "\n",
    "Raw data in the real-world is often *messy* and before we conduct any analysis on the data, we must first make it *tidy*. Cleaning data can be a tedious and repetitive task, and handling this problem inefficiently may lead to great exasperation.\n",
    "\n",
    "It is for these reasons that data preparation has often been labeled by various practitioners as\n",
    "\n",
    ">  the *least sexy, most time & labour intensive* task in data science.\n",
    "\n",
    "<br> \n",
    "\n",
    "<center>\n",
    "<img src=\"./images/ds_time_spent.png\" width=\"900px\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Tidy Data\n",
    "\n",
    "If you've ever taken a MOOC or undergraduate course in data analysis, chances are that you received a neatly formatted csv file to begin with. In the real world however, that almost never happens.\n",
    "\n",
    "Here are some of the more commonly observed patterns in unclean data.\n",
    "\n",
    "* Column names are missing or gibberish \n",
    "* Data from multiple variables are concatenated into a single column\n",
    "* **Missing data** are encoded in different ways \n",
    "* Features are stored using varying units of measurement\n",
    "* **Outliers** \n",
    "\n",
    "There are **strategies** to deal with each one of these patterns. The following sections will introuce you to the tools and techniques you can use to perform these janitorial duties. Most of these tasks will be performed in Python using the `pandas` library.\n",
    "\n",
    "Once the data is clean it may still require more work before it can be used to create meaningful visualizations or for building machine learning models. This is where we perform advanced *data wrangling* tasks such as \n",
    "\n",
    "* **Reshaping** long-data to wide-data \n",
    "* **Subsetting** data to retain relevant rows and/or columns \n",
    "* **Aggregating** data using the `split-apply-combine` strategy \n",
    "* **Sorting , Merging** to combine data from different sources\n",
    "\n",
    "If you persevere through these steps, you should have a `tidy` dataset that satisfies the criterion laid down by Hadley Wickham, and makes it easy to carry out data analysis. \n",
    "\n",
    "- Observations are in rows\n",
    "- Variables are in columns\n",
    "- Entities of one kind to be contained in a single dataset (ex. customers, transactions)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "# Pandas: Part 1\n",
    "\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Pandas** is a free, open-source data science library aimed at quick and simplified data munging and exploratory analysis in Python. <br><br> Specifically, it provides high-level data structures like the `DataFrame` (similar to the R `data.frame`) and `Series` (one-dimensional structure with an index) which have rich methods for tackling the entire spectrum of data munging tasks. Additionally, `pandas` has specialized methods for manipulating and visualizing numerical variables and time series data. \n",
    "\n",
    "Pandas creator Wes McKinney started building the library in 2008 during his time at an investment management firm. He was motivated by a need to address a distinct set of data analysis requirements that were not fully satisfiedby any one tool at his disposal at the time.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Python had long been great for data munging and preparation, but less so for data analysis and modeling. Pandas helps fill this gap, enabling you to carry out your entire data analysis workflow in Python without having to switch to a more domain specific language.\n",
    "\n",
    "<br>\n",
    "\n",
    "# Pandas, an introduction\n",
    "\n",
    "Pandas is built on top of numPy, and is designed to eliminate the need for writing loops for any filtering or aggregation work. It is implemented in C, so is around 15x faster than base python.\n",
    "\n",
    "Key Features\n",
    "\n",
    "* Easy handling of ***missing data.*** (`dropna, fillna, ffill, isnull, notnull`)\n",
    "\n",
    "* Simple ***mutations*** of tables (add/remove columns)\n",
    "\n",
    "* Easy ***slicing*** of data (fancy indexing and subsetting)\n",
    "\n",
    "* Automatic ***data alignment*** (by index)\n",
    "\n",
    "* Powerful ***split-apply-combine*** (`groupby`)\n",
    "\n",
    "* Intuitive ***merge/join*** (`concat, join`)\n",
    "\n",
    "* Reshaping and ***Pivoting*** (`stack, pivot`)\n",
    "\n",
    "* ***Hierarchical Labeling*** of axes indices\n",
    "\n",
    "* Robust ***I/O tools*** to work with csv, Excel, flat files, ***databases and HDFS***\n",
    "\n",
    "* Integrated ***Time Series*** Functionality\n",
    "\n",
    "* Easy plotting (`plot`)\n",
    "\n",
    "Pandas is built on a solid foundation of NumPy arrays, and is optimized for performance (pandas is about 15x faster), with essential code pieces written in Cython or C. NumPy’s `ndarray` and its broadcasting capabilities are leveraged extensively. \n",
    "\n",
    "The documentation is available [here](http://pandas.pydata.org)\n",
    "\n",
    "---\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "<center>\n",
    "\n",
    "# Getting Data into Pandas\n",
    "\n",
    "</center>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "We start, as always, by importing modules and frequently used classes from the module. \n",
    "As per convention, we write\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "```\n",
    "<br>\n",
    "\n",
    "---\n",
    "## Reading Flat Files\n",
    "\n",
    "`pandas` has functions such as `read_csv, read_table, read_fwf` and `read_clipboard` for reading tabular data into a `DataFrame` object. These functions take as arguments the following options:\n",
    "\n",
    "<br>\n",
    "\n",
    "*Which columns to consider?*   \n",
    "* Import the header (`header=`) or provide column names (`names=`)\n",
    " \n",
    "*Type inference and conversion*\n",
    "* Processing dates, combining date and time\n",
    "\n",
    "*Which column serves as the index?* (`index_col=`)\n",
    "* For a hierarchical index, pass a list of column names\n",
    "\n",
    "*Which values to interpret as missing data?* (`na_values=`)\n",
    "* If there are multiple sentinels for missing data, pass a dictionary\n",
    "\n",
    "*If the file is too large, read chunks iteratively* (`nrows=` and `chunksize=`)\n",
    "* This option is particularly helpful if you're working with files whose size runs into several gigabytes.\n",
    "\n",
    "Skipping over rows (`skiprows=`)\n",
    "* Sometimes data files have metadata in the first few lines before the actual data. \n",
    "\n",
    "Interpreting decimal numbers (points or commas to mark thousands)\n",
    "* Depending on where you are in the world, floating point numbers are written differently. For ex. The value of pi is written as 3.142 in India and most places, while in Europe you would write it as 3,142\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## CSV files\n",
    "\n",
    "Reading a CSV is as simple as calling the read_csv function. By default, the read_csv function expects the column separator to be a comma, but you can change that using the sep parameter.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```python\n",
    " pd.read_csv(filepath, sep=, header=, names=, skiprows=, na_values= ... ) \n",
    "```\n",
    "\n",
    "These functions are also able to pull data from a URL. Just use the URL in place of `filepath` \n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## Excel Files\n",
    "\n",
    "Pandas allows you to read and write Excel files, so you can easily read from Excel, write your code in Python, and then write back out to Excel - no need for VBA. Reading Excel files requires the xlrd library. \n",
    "\n",
    "You can install it via pip (`pip install xlrd`).\n",
    "\n",
    "```python\n",
    "football = pd.read_excel('path_to_excel_file.xlsx', 'sheet1')\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## SQL Databases\n",
    "\n",
    "`pandas` also has great support for reading or writing data directly from or to a database. The workflow consists of \n",
    "\n",
    "- *setting up a connection* to the database using add-on libraries like `sql-alchemy` and `psycopg2` that help in establishing connections to databases like RDBMS and Redshift. \n",
    "- *and then passing a SQL query over that connection* to the database. The query results, usually in tabular format, are returned to `pandas` as a `DataFrame` object.\n",
    "\n",
    "\n",
    "```python\n",
    "import sqlite3 \n",
    "\n",
    "conn = sqlite3.connect('my-sqlite-database.db')\n",
    "query = \"SELECT * FROM my-sqlite-database LIMIT 10;\"\n",
    "\n",
    "df = pd.read_sql(query, con=conn)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## Reading from the Clipboard\n",
    "\n",
    "Sometimes we need to quickly import data from an HTML table or an Excel sheet without needing to import the entire page or workbook. We could then just highlight the required data, hit `CTRL + C`, move over to pandas and run `pd.read_clipboard()`. It may be prudent, after the data is imported into your workspace, to back it up using DataFrame methods like `to_csv`\n",
    "\n",
    "```python\n",
    "df = pd.read_clipboard()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "## Using `bash` commands to peek at the data\n",
    "\n",
    "One handy feature of Jupyter is that you can issue shell commands from any cell by using the `!` \n",
    "\n",
    "This allows us to use shell functions like `head` and `wc` to glance at our data before importing it\n",
    "\n",
    "This way we can\n",
    "\n",
    "- decide which delimiter is appropriate\n",
    "- check if there are extra rows in the file before the table starts\n",
    "- see if there are any special characters and select an appropriate encoding\n",
    "- check if the file is too large and might need to be imported in chunks due to RAM constraints\n",
    "\n",
    "Example\n",
    "\n",
    "```bash\n",
    "wc -l mydata.csv\n",
    "head -n 10 mydata.csv\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now let's see some of these  in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Inspect file without importing it using bash commands\n",
    "\n",
    "# number of rows\n",
    "!wc -l ./data/titanic.csv "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# first 10 rows\n",
    "!head -10 ./data/titanic.csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df_titanic = pd.read_csv('./data/titanic.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df_titanic.tail(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "type(df_titanic)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Import data from sql\n",
    "\n",
    "conn = sqlite3.connect('./data/towed.db')\n",
    "query = \"SELECT * FROM towed WHERE make = 'FORD';\"\n",
    "\n",
    "df_towed = pd.read_sql(query, con=conn)\n",
    "df_towed.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "pd.read_sql(\"SELECT distinct make, count(*) as num_vehicles from towed group by 1 order by 2 desc limit 10\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "# Pandas Data Structures\n",
    "\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Pandas has two **workhorse** **data structures** – *Series* and *DataFrame* - that provide robust and easy-to-implement solutions to many data analysis tasks. These are built on top of NumPy arrays and inherit many of their methods and functions.\n",
    "\n",
    "Operations using these structures include \n",
    "\n",
    "||||||\n",
    "| ---------- | ------------- | ----------- | -------- | --------- |\n",
    "| **CREATE**     | **SUBSET**        | **INSERT**      | **UPDATE**   | **VIEW**      |\n",
    "| **FILTER** | **APPEND**    | **SORT**    | **JOIN** | **MERGE** |\n",
    "| **GROUP**  | **SUMMARIZE** | **RESHAPE** | **MAP**  | **APPLY** |\n",
    "\n",
    "\n",
    "## Series \n",
    "\n",
    "A `Series` is a one-dimensional, homogeneous array-like data structure containing a vector of data (of any valid NumPy type) and an associated array of data labels, called its *index*.\n",
    "\n",
    "### Creating a Series\n",
    "\n",
    "A Series can be created by calling the `pd.Series()` function on a NumPy 1D array or a Python collection like `list, tuple,` or `dictionary`. The index of the Series could be constructed in the same way from one of these objects. If the index isn't explicitly specified, a numeric vector from 0 to length-1 is automatically generated as the index. Additionally, the user may specify the *type* and a *name* while declaring a Series.\n",
    "\n",
    "**Syntax**\n",
    "\n",
    "```python\n",
    "pd.Series(data=, index=, dtype=, name=)\n",
    "```\n",
    "\n",
    "**Examples**\n",
    "\n",
    "```python\n",
    "# Basic Series with no explicit index from an array \n",
    "In []: series_1 = pd.Series(np.random.random(5).round(2))\n",
    "In []: series_1\n",
    "Out[]: \n",
    "0    0.03\n",
    "1    0.94\n",
    "2    0.69\n",
    "3    0.16\n",
    "4    0.61\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "Here, we created a series called *series_1* using an array of 5 elements filled with random numbers. The type was induced from the data to be *float64* and a numeric index was automatically generated.  Note that this series has no *name.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Series with an explicit index, dtype and name specification\n",
    "In []: series_2 = pd.Series(np.random.randint(0, 100, 5),\n",
    "                            index=list('abcde'),\n",
    "                            dtype=float,\n",
    "                            name='S2') \n",
    "In []: series_2\n",
    "Out[]: \n",
    "a    60.0\n",
    "b    13.0\n",
    "c    54.0\n",
    "d    13.0\n",
    "e    65.0\n",
    "Name: S2, dtype: float64\n",
    "```\n",
    "\n",
    "Here, we create a series of random integers, but convert the type to *float* explicitly. The automatic index is replaced by the one we provide. The name *S2* is important as it will become the column name if this `Series` is imported or concatenated into a `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# From a numeric list\n",
    "In []: pd.Series([-55, 4, 79, 101])\n",
    "Out[]: \n",
    "0    -55\n",
    "1      4\n",
    "2     79\n",
    "3    101\n",
    "dtype: int64\n",
    "\n",
    "# From a tuple\n",
    "In []: pd.Series((1, 2, 3, 4))\n",
    "Out[]: \n",
    "0    1\n",
    "1    2\n",
    "2    3\n",
    "3    4\n",
    "dtype: int64\n",
    "\n",
    "# From a dictionary\n",
    "# Note that when we use a Python dict to create a Series, the keys become the index.\n",
    "In []: pd.Series({'a': 101, 'b': -55, 'c': 79, 'd': 4})\n",
    "Out[]: \n",
    "a    101\n",
    "b    -55\n",
    "c     79\n",
    "d      4\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's see this in action:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "x_random = np.random.randn(10).round(2)\n",
    "\n",
    "print(type(x_random))\n",
    "x_random"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Series(x_random, \n",
    "       name='my_series_1', \n",
    "       dtype=object, \n",
    "       index=['ind_' + str(i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "s_random = Series(x_random)\n",
    "# no index specified, numeric will be automatically generated\n",
    "\n",
    "print(type(s_random))\n",
    "s_random"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# passing an index specifically\n",
    "my_series = Series(x_random, index=list('aabbbccdef'))\n",
    "\n",
    "my_series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Using a list, tuple or dict\n",
    "\n",
    "dict_1 = {v: k for k, v in zip(np.random.random(10).round(2), list('abcdefghij'))}\n",
    "dict_1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Series(dict_1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Series(data=[1, 2, 3], \n",
    "       index=list('abc'), \n",
    "       name='Series_1', \n",
    "       dtype=float)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Series(data=(1, 2, 3), \n",
    "       index=list('abc'), \n",
    "       name='Series_1', \n",
    "       dtype=np.int64)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Series({'a': 1, 'b': 2, 'c':3}, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Series Attributes\n",
    "\n",
    "Just like attributes for primitive Python data structures like `Lists` or `Dictionaries` provide useful metadata about the contents of the data structure, we can use `Series` attributes like `values,  index, shape` to get the underlying array, the index and the length of the series respectively.\n",
    "\n",
    "```python\n",
    "In []: series_2\n",
    "Out[]:\n",
    "a    34.0\n",
    "b    60.0\n",
    "c    21.0\n",
    "d    22.0\n",
    "e     7.0\n",
    "Name: S2, dtype: float64\n",
    "        \n",
    "# Get the underlying NumPy array\n",
    "In []: series_2.values\n",
    "Out[]: array([ 34.,  60.,  21.,  22.,   7.])\n",
    "\n",
    "# Get the index\n",
    "In []: series_2.index\n",
    "Out[]: Index([u'a', u'b', u'c', u'd', u'e'], dtype='object')\n",
    "\n",
    "# Get the size on disk\n",
    "In []: series_2.nbytes\n",
    "Out[]: 40\n",
    "\n",
    "# Get the number of elements\n",
    "In []: series_2.shape\n",
    "Out[]: (5,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting a Series\n",
    "\n",
    "There are many ways to extract subsets of a Series in Pandas. In addition to allowing NumPy-like subsetting using integer lists and slices, it is possible to subset a Series using \n",
    "\n",
    "* *label-based indexing* by passing index labels associated with the values\n",
    "* *fancy indexing* using methods like `loc, iloc, ix, at, iat`\n",
    "* *boolean indexing* for subsetting with logical arrays\n",
    "\n",
    "Example: Label and integer based indexing on a *Breakfast Menu* \n",
    "\n",
    "```python\n",
    "In [16]: menu = Series({'ham': 1, 'eggs': 3, 'bacon': 2, \n",
    "                        'coffee': 1, 'toast': 0.5, 'jam': 0.2})\n",
    "In [17]: menu\n",
    "Out[17]: \n",
    "bacon     2.0\n",
    "coffee    1.0\n",
    "eggs      3.0\n",
    "ham       1.0\n",
    "jam       0.2\n",
    "toast     0.5\n",
    "dtype: float64\n",
    "\n",
    "# A single label\n",
    "In [18]: menu['bacon']\n",
    "Out[18]: 2.0\n",
    "\n",
    "# A list of labels    \n",
    "In [19]: menu[['eggs', 'bacon']]\n",
    "Out[19]: \n",
    "eggs     3.0\n",
    "bacon    2.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "```python\n",
    "# A slice of labels\n",
    "In [20]: menu['bacon':'eggs']\n",
    "Out[20]: \n",
    "bacon     2.0\n",
    "coffee    1.0\n",
    "eggs      3.0\n",
    "dtype: float64\n",
    "\n",
    "# Another label slice\n",
    "In [21]: menu['ham':]\n",
    "Out[21]: \n",
    "ham      1.0\n",
    "jam      0.2\n",
    "toast    0.5\n",
    "dtype: float64\n",
    "\n",
    "# Positional Slicing\n",
    "In [22]: menu[0:3]\n",
    "Out[22]: \n",
    "bacon     2.0\n",
    "coffee    1.0\n",
    "eggs      3.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "### Subsetting with *fancy indexing* methods  \n",
    "\n",
    "  * `.loc()  ` for label based subsetting\n",
    "  * `.iloc()` for integer based subsetting\n",
    "\n",
    "  Note that other methods such as `.ix()` and `.at(), .iat()` exist, but they serve the same purpose. So we will leave it to the reader to explore these functions in the documentation and pick the one they feel most productive in using. \n",
    "\n",
    "```python\n",
    "# Using loc\n",
    "In [30]: menu.loc['coffee']\n",
    "Out[30]: 1.0\n",
    "\n",
    "In [31]: menu.loc['eggs':'jam']\n",
    "Out[31]: \n",
    "eggs    3.0\n",
    "ham     1.0\n",
    "jam     0.2\n",
    "dtype: float64\n",
    "\n",
    "# Using iloc    \n",
    "In [32]: menu.iloc[3]\n",
    "Out[32]: 1.0\n",
    "\n",
    "In [33]: menu.iloc[2:4]\n",
    "Out[33]: \n",
    "eggs    3.0\n",
    "ham     1.0\n",
    "dtype: float64\n",
    "\n",
    "```\n",
    "\n",
    "### Boolean indexing \n",
    "\n",
    "- Works in the same way as it does for subsetting NumPy arrays. \n",
    "\n",
    "- We create a boolean of the same length as the Series, (using the same Series), and then pass the boolean to the squre bracket subsetter. \n",
    "\n",
    "```python\n",
    "# Create a boolean series using a logical comparison\n",
    "In [35]: menu > 1\n",
    "Out[35]: \n",
    "bacon      True\n",
    "coffee    False\n",
    "eggs       True\n",
    "ham       False\n",
    "jam       False\n",
    "toast     False\n",
    "dtype: bool\n",
    "\n",
    "# Subset the series using the boolean to retain values where True\n",
    "In [36]: menu[menu > 1]\n",
    "Out[36]: \n",
    "bacon    2.0\n",
    "eggs     3.0\n",
    "dtype: float64\n",
    "\n",
    "# We can also pass any arbitrary boolean (needs to be of the same length)     \n",
    "In [37]: menu[[True, True, False, False, True, False]]\n",
    "Out[37]: \n",
    "bacon     2.0\n",
    "coffee    1.0\n",
    "jam       0.2\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Practice subsetting with the code below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "my_series = Series(np.random.randn(5).round(2), index = list('abcde'))\n",
    "my_series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# One Label\n",
    "my_series['a']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# List of Labels\n",
    "my_series[['a', 'b']] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Label Slice\n",
    "my_series['b':'d']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# positional slicing\n",
    "my_series[:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "my_series[1:4]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "my_series[::-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "my_series[::-2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "my_series.loc[['a', 'c', 'e']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "my_series.iloc[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "## Series Methods\n",
    "\n",
    "We've just seen a few `Series` methods that allow us to subset data. There's a variety of other methods that are useful across the entire spectrum of data wrangling tasks. The figure below shows an exhaustive list of all methods. We will discuss some of the most important ones of these here, and encourage the readers to peruse the latest pandas documentation to explore other methods.\n",
    "\n",
    "![80% center](./images/Series_methods.png)\n",
    "\n",
    "It is impossible to remember each by heart, so we will rely on our old friend `?` to ask for help\n",
    "\n",
    "`pd.Series.<unknown-method>?`\n",
    "\n",
    "\n",
    "---\n",
    "### Peeking at the data\n",
    "\n",
    "* `head and tail`  are used to view a small sample of a Series or DataFrame object, use the [`head()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html#pandas.DataFrame.head) and [`tail()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html#pandas.DataFrame.tail) methods. The default number of elements to display is five, but you may pass a custom number.\n",
    "\n",
    "```python\n",
    "In [44]: series_8 = Series(np.random.randn(1000).round(2))\n",
    "\n",
    "In [45]: series_8.head()\n",
    "Out[45]: \n",
    "0   -0.23\n",
    "1    0.55\n",
    "2    0.77\n",
    "3    0.18\n",
    "4    0.76\n",
    "dtype: float64\n",
    "\n",
    "In [46]: series_8.tail()\n",
    "Out[46]: \n",
    "995    0.66\n",
    "996   -0.48\n",
    "997    1.13\n",
    "998    1.05\n",
    "999    1.61\n",
    "dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Type Conversion\n",
    "\n",
    "* `astype` explicitly convert dtypes from one to another\n",
    "\n",
    "```python\n",
    "In [51]: series_8.head()\n",
    "Out[51]: \n",
    "0   -0.23\n",
    "1    0.55\n",
    "2    0.77\n",
    "3    0.18\n",
    "4    0.76\n",
    "dtype: float\n",
    "\n",
    "In [52]: series_8.astype(str).head()\n",
    "Out[52]: \n",
    "0    -0.23\n",
    "1     0.55\n",
    "2     0.77\n",
    "3     0.18\n",
    "4     0.76\n",
    "dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Treating Outliers\n",
    "\n",
    "* `clip()` can be used to clip outliers at a threshold value. All values lower than the one supplied to `lower=`, or higher than the one supplied to `upper=` will be replaced by that value. Note how the values in *series_8* below are clipped at the supplied threshold of `0.50`\n",
    "\n",
    "```python\n",
    "In [54]: series_8.head().clip(upper=.50)\n",
    "Out[54]: \n",
    "0   -0.23\n",
    "1    0.50\n",
    "2    0.50\n",
    "3    0.18\n",
    "4    0.50\n",
    "dtype: float64\n",
    "\n",
    "In [55]: series_8.head().clip(lower=.50)\n",
    "Out[55]: \n",
    "0    0.50\n",
    "1    0.55\n",
    "2    0.77\n",
    "3    0.50\n",
    "4    0.76\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "This function is especially useful in **treating outliers** when used in conjunction with `.quantile()` \n",
    "**Note**: In data wrangling, we generally clip values at the 1st-99th Percentile (or the 5th-95th percentile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Replacing Values\n",
    "\n",
    "* `replace` is an effective way to replace source values with target values by suppling a dictionary with the required substitutions. \n",
    "\n",
    "```python\n",
    "In [60]: fruits = Series(['apples', 'oranges', 'peaches', 'mangoes'])\n",
    "In [61]: fruits\n",
    "Out[61]: \n",
    "0     apples\n",
    "1    oranges\n",
    "2    peaches\n",
    "3    mangoes\n",
    "dtype: object\n",
    "\n",
    "In [62]: fruits.replace({'apples':'grapes', 'peaches':'bananas'})\n",
    "Out[62]: \n",
    "0     grapes\n",
    "1    oranges\n",
    "2    bananas\n",
    "3    mangoes\n",
    "dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Checking if values belong to a list\n",
    "\n",
    "* `isin` produces a boolean by comparing each element of the Series against the provided list. It takes `True` if the element belongs to the list. This boolean may then be used for subsetting the Series. \n",
    "\n",
    "```python\n",
    "In [64]: fruits.isin(['mangoes', 'oranges'])\n",
    "Out[64]: \n",
    "0    False\n",
    "1     True\n",
    "2    False\n",
    "3     True\n",
    "dtype: bool\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finding uniques and their frequency\n",
    "\n",
    "* `unique, nunique, value_counts` These methods are used to find the array of distinct values in a categorical Series, to count the number of distinct items, and to create a frequency table respectively.\n",
    "\n",
    "```python\n",
    "In [68]: series_9 = Series(list('abcd' * 3))\n",
    "In [69]: series_9\n",
    "Out[69]: \n",
    "0     a\n",
    "1     b\n",
    "2     c\n",
    "3     d\n",
    "4     a\n",
    "5     b\n",
    "6     c\n",
    "7     d\n",
    "8     a\n",
    "9     b\n",
    "10    c\n",
    "11    d\n",
    "dtype: object\n",
    "\n",
    "In [70]: series_9.unique()\n",
    "Out[70]: array(['a', 'b', 'c', 'd'], dtype=object)\n",
    "\n",
    "In [71]: series_9.nunique()\n",
    "Out[71]: 4\n",
    "    \n",
    "In [72]: series_9.value_counts()\n",
    "Out[72]: \n",
    "d    3\n",
    "b    3\n",
    "c    3\n",
    "a    3\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "All three of these methods prove indespensible while performing visualization and exploratory data analysis (EDA) tasks. \n",
    "For ex. The output produced by `value_counts()` helps us plot bar-charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dealing with Duplicates\n",
    "\n",
    "* `duplicated` produces a boolean that marks every instance of a value after its first occurrence as True. `drop_duplicates` returns the Series with the duplicates removed. \n",
    "  * If you want to drop duplicated permanently, pass the `inplace=True` argument.\n",
    "\n",
    "```python\n",
    "In [88]: series_9.duplicated()\n",
    "Out[88]: \n",
    "0     False\n",
    "1     False\n",
    "2     False\n",
    "3     False\n",
    "4      True\n",
    "5      True\n",
    "6      True\n",
    "7      True\n",
    "8      True\n",
    "9      True\n",
    "10     True\n",
    "11     True\n",
    "dtype: bool\n",
    "\n",
    "In [89]: series_9.drop_duplicates()\n",
    "Out[89]: \n",
    "0    a\n",
    "1    b\n",
    "2    c\n",
    "3    d\n",
    "dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sorting the data\n",
    "\n",
    "* `sort_values`, `sort_index` help in sorting a Series by values or by index in desired order. The `ascending=` argument controls the nature of the sort. \n",
    "\n",
    "  >  *[Note] In order to make the sorting permanent, we need to pass the `inplace=True` argument. The default value `inplace=False` returns a copy of the Series with the changes made, but the original remains intact. Many other methods like `drop_duplicates` also take this paramter.*\n",
    "\n",
    "```python\n",
    "In [82]: series_10.sort_values()\n",
    "Out[82]: \n",
    "y     5\n",
    "c     5\n",
    "a    12\n",
    "x    21\n",
    "z    28\n",
    "b    45\n",
    "dtype: int64    \n",
    "\n",
    "In [83]: series_10.sort_index()\n",
    "Out[83]: \n",
    "a    12\n",
    "b    45\n",
    "c     5\n",
    "x    21\n",
    "y     5\n",
    "z    28\n",
    "dtype: int64\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finding the largest/smallest values\n",
    "\n",
    "* `idxmax, idxmin, nlargest, nsmallest` As their names imply, these methods help in finding the largest, smallest, n-largest and n-smallest respectively. Note that the index label is returned with these values, and this can be especially helpful in EDA.\n",
    "\n",
    "```python\n",
    "In [76]: series_10 = Series(np.random.randint(0, 50, 6), index=list('xyzabc'))\n",
    "\n",
    "In [77]: series_10\n",
    "Out[77]: \n",
    "x    21\n",
    "y     5\n",
    "z    28\n",
    "a    12\n",
    "b    45\n",
    "c     5\n",
    "dtype: int64\n",
    "\n",
    "In [78]: series_10.idxmax()\n",
    "Out[78]: 'b'\n",
    "\n",
    "In [79]: series_10.idxmin()\n",
    "Out[79]: 'y'\n",
    "\n",
    "In [80]: series_10.nlargest(2)\n",
    "Out[80]: \n",
    "b    45\n",
    "z    28\n",
    "dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Mathematical Summaries\n",
    "\n",
    "* `mean, median, std, quantile, describe ` are mathematical methods employed to find the measures of central tendency for a given set of data points. `quantile` finds the requested percentiles, whereas `describe` produces the summary statistics for the data.\n",
    "  These functions come in handy when we're exploring data for patterns.\n",
    "\n",
    "```python\n",
    "# Create a long series with numeric values\n",
    "In [108]: series_11 = Series(np.random.randn(1000))\n",
    "\n",
    "In [109]: series_11.head()\n",
    "Out[109]: \n",
    "0   -0.808280\n",
    "1   -0.361064\n",
    "2    1.098265\n",
    "3   -0.400104\n",
    "4   -0.401763\n",
    "dtype: float64\n",
    "\n",
    "In [110]: series_11.mean()\n",
    "Out[110]: 0.010034515870708844\n",
    "\n",
    "In [111]: series_11.std()\n",
    "Out[111]: 0.9999153362726881\n",
    "\n",
    "In [112]: series_11.median()\n",
    "Out[112]: 0.008293242730166963\n",
    "\n",
    "# Find the 10th, 50th and 80th percentile    \n",
    "In [113]: series_11.quantile([0.10, 0.50, 0.80])\n",
    "Out[113]: \n",
    "0.1   -1.290976\n",
    "0.5    0.008293\n",
    "0.8    0.854793\n",
    "dtype: float64\n",
    "\n",
    "# Get all summary statistics\n",
    "In [114]: series_11.describe()\n",
    "Out[114]: \n",
    "count    1000.000000\n",
    "mean        0.010035\n",
    "std         0.999915\n",
    "min        -3.095835\n",
    "25%        -0.665170\n",
    "50%         0.008293\n",
    "75%         0.693991\n",
    "max         3.762116\n",
    "dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "\n",
    "* `isnull, notnull ` are complementary methods that work on a Series with missing data to produce a boolean Series that identifies missing or non-missing values respectively. \n",
    "\n",
    ">  Note that both the NumPy `np.nan` and the base Python `None` type are identified as missing values.\n",
    "\n",
    "```python\n",
    "In [93]: series_12 = Series([1.12, 3.14, np.nan, 6.02, 2.73, None])\n",
    "\n",
    "In [94]: series_12\n",
    "Out[94]: \n",
    "0    1.12\n",
    "1    3.14\n",
    "2     NaN\n",
    "3    6.02\n",
    "4    2.73\n",
    "5     NaN\n",
    "dtype: float64\n",
    "\n",
    "# Mark the missing values as True\n",
    "In [95]: series_12.isnull()\n",
    "Out[95]: \n",
    "0    False\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "4    False\n",
    "5     True\n",
    "dtype: bool\n",
    "\n",
    "# Mark the non-missing values as True     \n",
    "In [96]: series_12.notnull()\n",
    "Out[96]: \n",
    "0     True\n",
    "1     True\n",
    "2    False\n",
    "3     True\n",
    "4     True\n",
    "5    False\n",
    "dtype: bool\n",
    "```\n",
    "\n",
    "These functions may be used in conjunction with `sum` and `mean` to find the number or proportion of missing values in each variable of the imported data. Generally, variables with more than 70- 80% missing values are not useful in data analysis.\n",
    "\n",
    "```python\n",
    "# find number of missing values\n",
    "series_12.isnull().sum()\n",
    "\n",
    "# find percentage of missing values\n",
    "series_12.isnull().mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Missing Values Imputation\n",
    "\n",
    "* `fillna, ffill and bfill, dropna` This set of Series methods allow us to deal with missing data by choosing to either impute them with a particular value, or by copying the last known value over the missing ones (typically used in time-series analysis.) \n",
    "  We may sometimes want to drop the missing data altogether and `dropna` helps us in doing that.\n",
    "\n",
    "> [Note] It is a common practice in data science to replace missing values in a numeric variable by its mean (or median if the data is skewed) and in categorical variables with its mode.\n",
    "\n",
    "```python\n",
    "In [118]: series_12 = Series([1.12, 3.14, np.nan, 6.02, 2.73, None])\n",
    "\n",
    "# Replace missings with 0\n",
    "In [119]: series_12.fillna(0)\n",
    "Out[119]: \n",
    "0    1.12\n",
    "1    3.14\n",
    "2    0.00s\n",
    "3    6.02\n",
    "4    2.73\n",
    "5    0.00\n",
    "dtype: float64\n",
    "\n",
    "# Replace missings with the mean    \n",
    "In [120]: series_12.fillna(series_12.mean())\n",
    "Out[120]: \n",
    "0    1.1200\n",
    "1    3.1400\n",
    "2    3.2525\n",
    "3    6.0200\n",
    "4    2.7300\n",
    "5    3.2525\n",
    "dtype: float64\n",
    "\n",
    "# Replicate the last non-missing value over the missing ones    \n",
    "In [121]: series_12.ffill()\n",
    "Out[121]: \n",
    "0    1.12\n",
    "1    3.14\n",
    "2    3.14\n",
    "3    6.02\n",
    "4    2.73\n",
    "5    2.73\n",
    "dtype: float64\n",
    "\n",
    "# Return a copy of the Series with missing values removed    \n",
    "In [122]: series_12.dropna()\n",
    "Out[122]: \n",
    "0    1.12\n",
    "1    3.14\n",
    "3    6.02\n",
    "4    2.73\n",
    "dtype: float64    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Apply a function to each element\n",
    "\n",
    "*  `map` is perhaps the **most important** of all Series methods. It takes a *general-purpose* or *user-defined* function and applies it to each value in the Series. Combined with base Python's *lambda functions*, it can be an incredibly powerful tool in transforming a given Series.\n",
    "\n",
    "\n",
    "```python\n",
    "# Let's say we have a list of names stored in a Series\n",
    "In [125]: series_13 = Series(['Dave Smith', 'Jane Doe', 'Carl James', 'Jim Hunt'])\n",
    "\n",
    "In [126]: series_13\n",
    "Out[126]: \n",
    "0    Dave Smith\n",
    "1      Jane Doe\n",
    "2    Carl James\n",
    "3      Jim Hunt\n",
    "dtype: object\n",
    "\n",
    "# Find the length of each name    \n",
    "In [126]: series_13.map(lambda x: len(x))\n",
    "Out[126]: \n",
    "0    10\n",
    "1     8\n",
    "2    10\n",
    "3     8\n",
    "dtype: int64\n",
    "\n",
    "# Find the initials    \n",
    "In [127]: series_13.map(lambda x: '.'.join([i[0] for i in x.split(' ')]))\n",
    "Out[127]: \n",
    "0    D.S\n",
    "1    J.D\n",
    "2    C.J\n",
    "3    J.H\n",
    "dtype: object    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "\n",
    "* The `plot` method is a gateway to a treasure trove of potential visualizations like histograms, bar charts, scatterplots, boxplots and more. As examples, we will visualize a bar chart for a categorical series and the histogram of a numeric variable.\n",
    "\n",
    "```python\n",
    "# Create a categorical series\n",
    "In [148]: series_14 = Series(list('a' * 3) + list('b' * 5) + list('c' * 9) + list('d' * 2))\n",
    "    \n",
    "In [150]: series_14.value_counts().plot.bar()    \n",
    "```\n",
    "\n",
    "![](./images/series_plot_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "In [151]: series_15 = Series(np.random.randn(1000))\n",
    "\n",
    "In [151]: series_15.head()\n",
    "Out[151]: \n",
    "0    1.796526\n",
    "1    0.323100\n",
    "2   -1.747839\n",
    "3   -0.435137\n",
    "4    0.182139\n",
    "dtype: float64\n",
    "\n",
    "In [152]: series_15.plot.hist()\n",
    "```\n",
    "\n",
    "![](./images/series_plot_1.png)\n",
    "\n",
    "There are extensive customiztions that we can make to the aesthetics of these plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Difference between None and NaN\n",
    "\n",
    "- `NaN` is a mathematical entity\n",
    "- `None` is for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(np.nan)\n",
    "# Truthiness value of np.nan is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     True\n",
       "c    False\n",
       "b     True\n",
       "d    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series Methods do not discriminate between None and NaN\n",
    "Series({'a': None, 'c': 101, 'b': np.nan, 'd': 'red'}).isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Practice "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "cities = Series(data = [18, None, 5, None, 13], \n",
    "                index=['DEL', 'BOM', 'BLR', 'DXB', 'BKK'])\n",
    "cities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "list(zip(cities, cities.isnull()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "list(zip(cities, cities.notnull()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "cities[cities.isnull()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "cities[cities.notnull()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "cities.ffill()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "cities.fillna(cities.median())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Describe method on char series\n",
    "Series(list('Dogs are descended from wolves.')).describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Describe method on numeric series\n",
    "ser_x = Series(np.random.normal(0, 1, 10000))\n",
    "ser_x.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ser_x.describe(percentiles=[0.01, 0.05, 0.97, 0.99], )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ser_x.describe().loc[['min', 'mean', '50%', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: More Series methods using the Titanic Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df_x = pd.read_csv('./data/titanic.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df_x.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "type(df_x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "type(df_x.loc[:, 'Survived'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "type(df_x.loc[0, :])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# value_counts() for frequency tables\n",
    "df_x['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# type conversion\n",
    "df_x['PassengerId'].head().astype(float).astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# applying a function to each element of a series\n",
    "df_x['Fare'].head().map(lambda x: 'AED ' + str(int(x * 3.9)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df_x['Sex'].head().map(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# finding where the maximum value occured\n",
    "fares = Series(df_x['Fare'].values, index=df_x['Name'].values)\n",
    "\n",
    "print(f\"{fares.idxmax()} paid the highest fare of {fares.max()}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sort a series\n",
    "fares.sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Task\n",
    "\n",
    "Generate a Series of 150 ages with a mean of 35 years. Set every fifteenth value to missing. Find the nexw mean. Fill the missing data with (a) mean (b) median, and report the new means\n",
    "\n",
    "hint: use `np.random.randn`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "%load ./scripts/task_missings.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
